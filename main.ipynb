{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, torch \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torch.utils.data import DataLoader \n",
    "import torch.optim as optim \n",
    "from model import generator, multi_period_discriminator, multi_scale_discriminator, feature_loss, generator_loss, discriminator_loss\n",
    "import wandb \n",
    "import pandas as pd \n",
    "import itertools\n",
    "import dataset \n",
    "import sounddevice as sd \n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "if device == 'cuda': print(torch.cuda.get_device_name())\n",
    "else: print('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram): \n",
    "  fig, ax = plt.subplots(figsize=(10, 2)) \n",
    "  im = ax.imshow(spectrogram, aspect='auto', origin='lower', interpolation='none')\n",
    "  plt.colorbar(im, ax=ax) \n",
    "  fig.canvas.draw() \n",
    "  plt.close() \n",
    "  return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr(dict): \n",
    "  def __init__(self, *args, **kwargs): \n",
    "    super(Attr, self).__init__(*args, **kwargs) \n",
    "    self.__dict__ = self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'rb') as file: \n",
    "  config = json.load(file)\n",
    "config = Attr(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = generator(config).to(device) \n",
    "mpd = multi_period_discriminator().to(device) \n",
    "msd = multi_scale_discriminator().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_g = optim.AdamW(generator_model.parameters(), config.learning_rate, betas=[config.adam_b1, config.adam_b2])\n",
    "optim_d = optim.AdamW(itertools.chain(msd.parameters(), mpd.parameters()), config.learning_rate, betas=[config.adam_b1, config.adam_b2])\n",
    "\n",
    "scheduler_g = optim.lr_scheduler.ExponentialLR(optim_g, gamma=config.lr_decay)\n",
    "scheduler_d = optim.lr_scheduler.ExponentialLR(optim_d, gamma=config.lr_decay) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_base = '/mnt/sda1/data/lj_speech/LJSpeech-1.1/wavs/'\n",
    "all_files = [os.path.join(data_path_base, i) for i in os.listdir('/mnt/sda1/data/lj_speech/LJSpeech-1.1/wavs')]\n",
    "cutoff = int(0.9 * len(all_files))\n",
    "training_filelist = all_files[:cutoff]\n",
    "testing_filelist = all_files[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = dataset.mel_dataset(training_filelist, config.segment_size, config.n_fft, config.num_mels, \n",
    "                               config.hop_size, config.win_size, config.sampling_rate, \n",
    "                               config.fmin, config.fmax, n_cache_reuse=0, \n",
    "                               shuffle=True, fmax_loss=config.fmax_for_loss, device=device, \n",
    "                               fine_tuning=False, base_mels_path=data_path_base)\n",
    "testset = dataset.mel_dataset(testing_filelist, config.segment_size, config.n_fft, config.num_mels, \n",
    "                               config.hop_size, config.win_size, config.sampling_rate, \n",
    "                               config.fmin, config.fmax, split=False, n_cache_reuse=0, \n",
    "                               shuffle=False, fmax_loss=config.fmax_for_loss, device=device, \n",
    "                               fine_tuning=False, base_mels_path=data_path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(trainset, num_workers=config.num_workers, shuffle=False, batch_size=config.batch_size, pin_memory=True, drop_last=True)\n",
    "test_dl = DataLoader(testset, num_workers=1, shuffle=False, batch_size=1, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muuzall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8752b9640aa840b9a5795cc960329523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668224550085143, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/sda1/Programming/hifigan/wandb/run-20230814_145909-s2g5p5sm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uuzall/HiFi%20GAN/runs/s2g5p5sm' target=\"_blank\">hardy-water-13</a></strong> to <a href='https://wandb.ai/uuzall/HiFi%20GAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uuzall/HiFi%20GAN' target=\"_blank\">https://wandb.ai/uuzall/HiFi%20GAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uuzall/HiFi%20GAN/runs/s2g5p5sm' target=\"_blank\">https://wandb.ai/uuzall/HiFi%20GAN/runs/s2g5p5sm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project = 'HiFi GAN', \n",
    "    entity='uuzall', \n",
    "    sync_tensorboard=True, \n",
    "    name=''\n",
    ")\n",
    "writer = SummaryWriter(f'runs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(global_step): \n",
    "  generator_model.eval() \n",
    "  torch.cuda.empty_cache() \n",
    "  val_err_tot = 0 \n",
    "  with torch.no_grad(): \n",
    "    for j, batch in enumerate(test_dl): \n",
    "      x, y, _, y_mel = batch \n",
    "      y_g_hat = generator_model(x.permute(0, 2, 1).to(device)) \n",
    "      y_mel = y_mel.to(device) \n",
    "      y_g_hat_mel = dataset.mel_spectrogram(y_g_hat.squeeze(1), config.n_fft, config.num_mels, config.sampling_rate, \n",
    "                                            config.hop_size, config.win_size, config.fmin, config.fmax_for_loss) \n",
    "      val_err_tot += F.l1_loss(y_mel, y_g_hat_mel.permute(2, 0, 1)).item() \n",
    "\n",
    "      if j <= 4: \n",
    "        if global_step == 0: \n",
    "          writer.add_audio(f'gt/y_{j}', y[0], global_step, config.sampling_rate) \n",
    "          writer.add_figure(f'gt/y_spec_{j}', plot_spectrogram(x[0]), global_step) \n",
    "        \n",
    "        writer.add_audio(f'generated/y_hat_{j}', y_g_hat[0], global_step, config.sampling_rate)\n",
    "        y_hat_spec = dataset.mel_spectrogram(y_g_hat.squeeze(1), config.n_fft, config.num_mels, \n",
    "                                             config.sampling_rate, config.hop_size, config.win_size, \n",
    "                                             config.fmin, config.fmax) \n",
    "        writer.add_figure(f'generated/y_hat_spec_{j}', plot_spectrogram(y_hat_spec.squeeze(0).cpu().numpy()), global_step) \n",
    "  val_err_tot /= (j+1)\n",
    "  writer.add_scalar('testing/mel_spec_error', val_err_tot, global_step)\n",
    "  generator_model.train() \n",
    "  return val_err_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 736/736 [08:12<00:00,  1.49it/s, best_loss=0.641, current_test_loss=0.641, loss_disc_all=3.25, loss_gen_all=35.3]\n",
      "Epoch 2/100: 100%|██████████| 736/736 [07:16<00:00,  1.69it/s, best_loss=0.545, current_test_loss=0.545, loss_disc_all=3.26, loss_gen_all=29.9]  \n",
      "Epoch 3/100: 100%|██████████| 736/736 [07:51<00:00,  1.56it/s, best_loss=0.494, current_test_loss=0.508, loss_disc_all=3.65, loss_gen_all=29.2] \n",
      "Epoch 4/100: 100%|██████████| 736/736 [07:17<00:00,  1.68it/s, best_loss=0.478, current_test_loss=0.478, loss_disc_all=3.48, loss_gen_all=27.5]  \n",
      "Epoch 5/100: 100%|██████████| 736/736 [07:53<00:00,  1.55it/s, best_loss=0.453, current_test_loss=0.453, loss_disc_all=3.49, loss_gen_all=27.1] \n",
      "Epoch 6/100: 100%|██████████| 736/736 [07:16<00:00,  1.69it/s, best_loss=0.444, current_test_loss=0.444, loss_disc_all=3.51, loss_gen_all=28]    \n",
      "Epoch 7/100: 100%|██████████| 736/736 [07:58<00:00,  1.54it/s, best_loss=0.444, current_test_loss=0.454, loss_disc_all=3.34, loss_gen_all=25.2] \n",
      "Epoch 8/100: 100%|██████████| 736/736 [07:06<00:00,  1.73it/s, best_loss=0.422, current_test_loss=0.422, loss_disc_all=3.67, loss_gen_all=28.3]  \n",
      "Epoch 9/100: 100%|██████████| 736/736 [07:41<00:00,  1.60it/s, best_loss=0.403, current_test_loss=0.403, loss_disc_all=3.12, loss_gen_all=28.1]  \n",
      "Epoch 10/100: 100%|██████████| 736/736 [07:01<00:00,  1.75it/s, best_loss=0.402, current_test_loss=0.402, loss_disc_all=3.42, loss_gen_all=27.1]  \n",
      "Epoch 11/100: 100%|██████████| 736/736 [07:34<00:00,  1.62it/s, best_loss=0.399, current_test_loss=0.399, loss_disc_all=3.33, loss_gen_all=26.3]  \n",
      "Epoch 12/100: 100%|██████████| 736/736 [06:58<00:00,  1.76it/s, best_loss=0.399, current_test_loss=0.445, loss_disc_all=3.25, loss_gen_all=28]  \n",
      "Epoch 13/100: 100%|██████████| 736/736 [07:34<00:00,  1.62it/s, best_loss=0.397, current_test_loss=0.398, loss_disc_all=3.35, loss_gen_all=26.2]  \n",
      "Epoch 14/100: 100%|██████████| 736/736 [06:59<00:00,  1.76it/s, best_loss=0.397, current_test_loss=0.401, loss_disc_all=3.25, loss_gen_all=25]  \n",
      "Epoch 15/100: 100%|██████████| 736/736 [07:34<00:00,  1.62it/s, best_loss=0.389, current_test_loss=0.396, loss_disc_all=3.19, loss_gen_all=27.2]  \n",
      "Epoch 16/100: 100%|██████████| 736/736 [07:00<00:00,  1.75it/s, best_loss=0.381, current_test_loss=0.381, loss_disc_all=3, loss_gen_all=27.2]   \n",
      "Epoch 17/100: 100%|██████████| 736/736 [07:37<00:00,  1.61it/s, best_loss=0.372, current_test_loss=0.372, loss_disc_all=3.33, loss_gen_all=25.5]  \n",
      "Epoch 18/100: 100%|██████████| 736/736 [07:00<00:00,  1.75it/s, best_loss=0.372, current_test_loss=0.372, loss_disc_all=3.27, loss_gen_all=25.8]\n",
      "Epoch 19/100: 100%|██████████| 736/736 [07:00<00:00,  1.75it/s, best_loss=0.371, current_test_loss=0.371, loss_disc_all=2.73, loss_gen_all=28]    \n",
      "Epoch 20/100: 100%|██████████| 736/736 [07:35<00:00,  1.61it/s, best_loss=0.352, current_test_loss=0.352, loss_disc_all=2.87, loss_gen_all=27.5] \n",
      "Epoch 21/100: 100%|██████████| 736/736 [06:59<00:00,  1.75it/s, best_loss=0.352, current_test_loss=0.364, loss_disc_all=2.98, loss_gen_all=25.9]  \n",
      "Epoch 22/100: 100%|██████████| 736/736 [07:34<00:00,  1.62it/s, best_loss=0.35, current_test_loss=0.357, loss_disc_all=3.14, loss_gen_all=25.3]\n",
      "Epoch 23/100: 100%|██████████| 736/736 [06:58<00:00,  1.76it/s, best_loss=0.35, current_test_loss=0.353, loss_disc_all=3.16, loss_gen_all=26.4]  \n",
      "Epoch 24/100: 100%|██████████| 736/736 [07:34<00:00,  1.62it/s, best_loss=0.334, current_test_loss=0.347, loss_disc_all=3.04, loss_gen_all=25.8] \n",
      "Epoch 25/100: 100%|██████████| 736/736 [06:58<00:00,  1.76it/s, best_loss=0.334, current_test_loss=0.354, loss_disc_all=2.89, loss_gen_all=26.7]  \n",
      "Epoch 26/100: 100%|██████████| 736/736 [07:33<00:00,  1.62it/s, best_loss=0.334, current_test_loss=0.345, loss_disc_all=2.95, loss_gen_all=26.5]  \n",
      "Epoch 27/100: 100%|██████████| 736/736 [06:58<00:00,  1.76it/s, best_loss=0.334, current_test_loss=0.336, loss_disc_all=2.92, loss_gen_all=27.6]  \n",
      "Epoch 28/100: 100%|██████████| 736/736 [07:35<00:00,  1.62it/s, best_loss=0.332, current_test_loss=0.332, loss_disc_all=2.95, loss_gen_all=25.5]  \n",
      "Epoch 29/100: 100%|██████████| 736/736 [06:57<00:00,  1.76it/s, best_loss=0.332, current_test_loss=0.336, loss_disc_all=2.85, loss_gen_all=26.3]  \n",
      "Epoch 30/100: 100%|██████████| 736/736 [07:37<00:00,  1.61it/s, best_loss=0.332, current_test_loss=0.334, loss_disc_all=3.05, loss_gen_all=25.3]  \n",
      "Epoch 31/100: 100%|██████████| 736/736 [07:03<00:00,  1.74it/s, best_loss=0.323, current_test_loss=0.323, loss_disc_all=2.91, loss_gen_all=26.9]\n",
      "Epoch 32/100: 100%|██████████| 736/736 [07:16<00:00,  1.69it/s, best_loss=0.319, current_test_loss=0.325, loss_disc_all=3.23, loss_gen_all=24.9]  \n",
      "Epoch 33/100: 100%|██████████| 736/736 [06:31<00:00,  1.88it/s, best_loss=0.319, current_test_loss=0.331, loss_disc_all=2.88, loss_gen_all=26.5]\n",
      "Epoch 34/100: 100%|██████████| 736/736 [07:06<00:00,  1.73it/s, best_loss=0.31, current_test_loss=0.31, loss_disc_all=2.68, loss_gen_all=27.2]    \n",
      "Epoch 35/100: 100%|██████████| 736/736 [06:30<00:00,  1.88it/s, best_loss=0.31, current_test_loss=0.322, loss_disc_all=2.86, loss_gen_all=26.2]\n",
      "Epoch 36/100: 100%|██████████| 736/736 [06:30<00:00,  1.89it/s, best_loss=0.31, current_test_loss=0.321, loss_disc_all=2.99, loss_gen_all=24.9]  \n",
      "Epoch 37/100: 100%|██████████| 736/736 [07:01<00:00,  1.75it/s, best_loss=0.31, current_test_loss=0.314, loss_disc_all=2.87, loss_gen_all=26]  \n",
      "Epoch 38/100: 100%|██████████| 736/736 [06:28<00:00,  1.89it/s, best_loss=0.31, current_test_loss=0.316, loss_disc_all=2.95, loss_gen_all=25.5]  \n",
      "Epoch 39/100: 100%|██████████| 736/736 [07:01<00:00,  1.75it/s, best_loss=0.31, current_test_loss=0.329, loss_disc_all=2.84, loss_gen_all=27.3] \n",
      "Epoch 40/100: 100%|██████████| 736/736 [06:31<00:00,  1.88it/s, best_loss=0.31, current_test_loss=0.31, loss_disc_all=2.87, loss_gen_all=26.3]  \n",
      "Epoch 41/100: 100%|██████████| 736/736 [07:02<00:00,  1.74it/s, best_loss=0.31, current_test_loss=0.315, loss_disc_all=2.82, loss_gen_all=25.9] \n",
      "Epoch 42/100: 100%|██████████| 736/736 [06:29<00:00,  1.89it/s, best_loss=0.31, current_test_loss=0.316, loss_disc_all=2.79, loss_gen_all=25.6]  \n",
      "Epoch 43/100: 100%|██████████| 736/736 [07:02<00:00,  1.74it/s, best_loss=0.307, current_test_loss=0.307, loss_disc_all=2.86, loss_gen_all=25.3]\n",
      "Epoch 44/100: 100%|██████████| 736/736 [06:32<00:00,  1.88it/s, best_loss=0.3, current_test_loss=0.3, loss_disc_all=2.86, loss_gen_all=25.6]    \n",
      "Epoch 45/100: 100%|██████████| 736/736 [07:02<00:00,  1.74it/s, best_loss=0.3, current_test_loss=0.31, loss_disc_all=2.85, loss_gen_all=24.8]   \n",
      "Epoch 46/100: 100%|██████████| 736/736 [06:29<00:00,  1.89it/s, best_loss=0.3, current_test_loss=0.303, loss_disc_all=2.88, loss_gen_all=25.4]  \n",
      "Epoch 47/100: 100%|██████████| 736/736 [07:01<00:00,  1.75it/s, best_loss=0.3, current_test_loss=0.307, loss_disc_all=2.7, loss_gen_all=26]     \n",
      "Epoch 48/100: 100%|██████████| 736/736 [06:30<00:00,  1.89it/s, best_loss=0.3, current_test_loss=0.311, loss_disc_all=3.1, loss_gen_all=25.5] \n",
      "Epoch 49/100: 100%|██████████| 736/736 [07:00<00:00,  1.75it/s, best_loss=0.3, current_test_loss=0.303, loss_disc_all=2.7, loss_gen_all=26.1]   \n",
      "Epoch 50/100: 100%|██████████| 736/736 [06:29<00:00,  1.89it/s, best_loss=0.3, current_test_loss=0.303, loss_disc_all=2.96, loss_gen_all=24.6]\n",
      "Epoch 51/100: 100%|██████████| 736/736 [07:03<00:00,  1.74it/s, best_loss=0.293, current_test_loss=0.293, loss_disc_all=2.76, loss_gen_all=26]  \n",
      "Epoch 52/100: 100%|██████████| 736/736 [06:29<00:00,  1.89it/s, best_loss=0.293, current_test_loss=0.297, loss_disc_all=2.85, loss_gen_all=25.4]\n",
      "Epoch 53/100: 100%|██████████| 736/736 [07:13<00:00,  1.70it/s, best_loss=0.293, current_test_loss=0.299, loss_disc_all=2.7, loss_gen_all=27]     \n",
      "Epoch 54/100: 100%|██████████| 736/736 [06:41<00:00,  1.83it/s, best_loss=0.293, current_test_loss=0.298, loss_disc_all=3.07, loss_gen_all=24.7]\n",
      "Epoch 55/100: 100%|██████████| 736/736 [06:30<00:00,  1.88it/s, best_loss=0.293, current_test_loss=0.298, loss_disc_all=2.82, loss_gen_all=26.2]  \n",
      "Epoch 56/100: 100%|██████████| 736/736 [07:21<00:00,  1.67it/s, best_loss=0.291, current_test_loss=0.291, loss_disc_all=2.7, loss_gen_all=25.9]  \n",
      "Epoch 57/100: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.29, current_test_loss=0.29, loss_disc_all=2.66, loss_gen_all=26.3]  \n",
      "Epoch 58/100: 100%|██████████| 736/736 [07:10<00:00,  1.71it/s, best_loss=0.29, current_test_loss=0.299, loss_disc_all=2.81, loss_gen_all=25.1] \n",
      "Epoch 59/100: 100%|██████████| 736/736 [06:28<00:00,  1.89it/s, best_loss=0.29, current_test_loss=0.296, loss_disc_all=2.82, loss_gen_all=25.8]  \n",
      "Epoch 60/100: 100%|██████████| 736/736 [07:02<00:00,  1.74it/s, best_loss=0.283, current_test_loss=0.283, loss_disc_all=2.85, loss_gen_all=24.9]\n",
      "Epoch 61/100: 100%|██████████| 736/736 [06:28<00:00,  1.89it/s, best_loss=0.283, current_test_loss=0.292, loss_disc_all=2.8, loss_gen_all=24.4]   \n",
      "Epoch 62/100: 100%|██████████| 736/736 [06:59<00:00,  1.75it/s, best_loss=0.283, current_test_loss=0.286, loss_disc_all=2.77, loss_gen_all=25.6]  \n",
      "Epoch 63/100: 100%|██████████| 736/736 [06:35<00:00,  1.86it/s, best_loss=0.283, current_test_loss=0.289, loss_disc_all=3.19, loss_gen_all=25.2]  \n",
      "Epoch 64/100: 100%|██████████| 736/736 [07:41<00:00,  1.60it/s, best_loss=0.28, current_test_loss=0.287, loss_disc_all=2.82, loss_gen_all=25.4] \n",
      "Epoch 65/100: 100%|██████████| 736/736 [06:45<00:00,  1.82it/s, best_loss=0.28, current_test_loss=0.288, loss_disc_all=2.87, loss_gen_all=24.7]\n",
      "Epoch 66/100: 100%|██████████| 736/736 [07:09<00:00,  1.71it/s, best_loss=0.273, current_test_loss=0.273, loss_disc_all=2.82, loss_gen_all=25.2] \n",
      "Epoch 67/100: 100%|██████████| 736/736 [06:36<00:00,  1.86it/s, best_loss=0.273, current_test_loss=0.275, loss_disc_all=2.75, loss_gen_all=26.5]\n",
      "Epoch 68/100: 100%|██████████| 736/736 [07:17<00:00,  1.68it/s, best_loss=0.273, current_test_loss=0.275, loss_disc_all=2.91, loss_gen_all=24.9]  \n",
      "Epoch 69/100: 100%|██████████| 736/736 [06:52<00:00,  1.78it/s, best_loss=0.273, current_test_loss=0.279, loss_disc_all=2.69, loss_gen_all=25.2]\n",
      "Epoch 70/100: 100%|██████████| 736/736 [07:44<00:00,  1.58it/s, best_loss=0.273, current_test_loss=0.277, loss_disc_all=2.8, loss_gen_all=24.9]   \n",
      "Epoch 71/100: 100%|██████████| 736/736 [07:02<00:00,  1.74it/s, best_loss=0.273, current_test_loss=0.274, loss_disc_all=2.93, loss_gen_all=24.3]\n",
      "Epoch 72/100: 100%|██████████| 736/736 [06:58<00:00,  1.76it/s, best_loss=0.273, current_test_loss=0.273, loss_disc_all=2.67, loss_gen_all=26.1]  \n",
      "Epoch 73/100: 100%|██████████| 736/736 [07:16<00:00,  1.69it/s, best_loss=0.273, current_test_loss=0.28, loss_disc_all=2.63, loss_gen_all=25.8]  \n",
      "Epoch 74/100: 100%|██████████| 736/736 [06:37<00:00,  1.85it/s, best_loss=0.273, current_test_loss=0.274, loss_disc_all=2.83, loss_gen_all=25]    \n",
      "Epoch 75/100: 100%|██████████| 736/736 [07:24<00:00,  1.65it/s, best_loss=0.273, current_test_loss=0.273, loss_disc_all=2.82, loss_gen_all=24.9] \n",
      "Epoch 76/100: 100%|██████████| 736/736 [06:37<00:00,  1.85it/s, best_loss=0.268, current_test_loss=0.268, loss_disc_all=2.79, loss_gen_all=25]    \n",
      "Epoch 77/100: 100%|██████████| 736/736 [07:20<00:00,  1.67it/s, best_loss=0.267, current_test_loss=0.27, loss_disc_all=2.72, loss_gen_all=26.1]  \n",
      "Epoch 78/100: 100%|██████████| 736/736 [06:46<00:00,  1.81it/s, best_loss=0.267, current_test_loss=0.274, loss_disc_all=2.81, loss_gen_all=24.9]  \n",
      "Epoch 79/100: 100%|██████████| 736/736 [07:41<00:00,  1.60it/s, best_loss=0.263, current_test_loss=0.263, loss_disc_all=2.88, loss_gen_all=24.5] \n",
      "Epoch 80/100: 100%|██████████| 736/736 [07:13<00:00,  1.70it/s, best_loss=0.263, current_test_loss=0.285, loss_disc_all=2.69, loss_gen_all=25.5]  \n",
      "Epoch 81/100: 100%|██████████| 736/736 [07:52<00:00,  1.56it/s, best_loss=0.263, current_test_loss=0.27, loss_disc_all=2.76, loss_gen_all=24.7]   \n",
      "Epoch 82/100: 100%|██████████| 736/736 [07:13<00:00,  1.70it/s, best_loss=0.263, current_test_loss=0.275, loss_disc_all=2.75, loss_gen_all=25]    \n",
      "Epoch 83/100: 100%|██████████| 736/736 [07:39<00:00,  1.60it/s, best_loss=0.262, current_test_loss=0.264, loss_disc_all=2.59, loss_gen_all=26.2]  \n",
      "Epoch 84/100: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.262, current_test_loss=0.285, loss_disc_all=2.76, loss_gen_all=24.7]\n",
      "Epoch 85/100: 100%|██████████| 736/736 [07:24<00:00,  1.65it/s, best_loss=0.262, current_test_loss=0.263, loss_disc_all=2.61, loss_gen_all=25.4]  \n",
      "Epoch 86/100: 100%|██████████| 736/736 [06:52<00:00,  1.78it/s, best_loss=0.261, current_test_loss=0.261, loss_disc_all=2.68, loss_gen_all=24.6]\n",
      "Epoch 87/100: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.261, current_test_loss=0.268, loss_disc_all=2.79, loss_gen_all=24.8]  \n",
      "Epoch 88/100: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.261, current_test_loss=0.265, loss_disc_all=2.79, loss_gen_all=25.5]\n",
      "Epoch 89/100: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.261, current_test_loss=0.261, loss_disc_all=2.79, loss_gen_all=24.4]  \n",
      "Epoch 90/100: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.261, current_test_loss=0.265, loss_disc_all=2.72, loss_gen_all=25]  \n",
      "Epoch 91/100: 100%|██████████| 736/736 [06:52<00:00,  1.78it/s, best_loss=0.259, current_test_loss=0.259, loss_disc_all=2.69, loss_gen_all=25.8]  \n",
      "Epoch 92/100: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.259, current_test_loss=0.259, loss_disc_all=2.69, loss_gen_all=24.6] \n",
      "Epoch 93/100: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.259, current_test_loss=0.262, loss_disc_all=2.64, loss_gen_all=26.1]  \n",
      "Epoch 94/100: 100%|██████████| 736/736 [07:24<00:00,  1.65it/s, best_loss=0.259, current_test_loss=0.265, loss_disc_all=2.66, loss_gen_all=24.9] \n",
      "Epoch 95/100: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.259, current_test_loss=0.263, loss_disc_all=2.75, loss_gen_all=24.6]  \n",
      "Epoch 96/100: 100%|██████████| 736/736 [07:28<00:00,  1.64it/s, best_loss=0.258, current_test_loss=0.258, loss_disc_all=2.84, loss_gen_all=23.8] \n",
      "Epoch 97/100: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.258, current_test_loss=0.263, loss_disc_all=2.8, loss_gen_all=24.5]   \n",
      "Epoch 98/100: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.258, current_test_loss=0.258, loss_disc_all=2.69, loss_gen_all=24.7] \n",
      "Epoch 99/100: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.258, current_test_loss=0.258, loss_disc_all=2.91, loss_gen_all=23.1]  \n",
      "Epoch 100/100: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.258, current_test_loss=0.259, loss_disc_all=3.17, loss_gen_all=23.7]  \n"
     ]
    }
   ],
   "source": [
    "generator_model.train() \n",
    "mpd.train() \n",
    "msd.train() \n",
    "global_step = 0 \n",
    "n_epochs = 100\n",
    "best_loss = 1000\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "  for i, batch in (loop := tqdm(enumerate(train_dl), total=len(train_dl))): \n",
    "    x, y, _, y_mel = batch \n",
    "    x, y, y_mel = x.permute(0, 2, 1).to(device), y.unsqueeze(1).to(device), y_mel.to(device)\n",
    "\n",
    "    y_g_hat = generator_model(x) \n",
    "    y_g_hat_mel = dataset.mel_spectrogram(y_g_hat.squeeze(1), config.n_fft, config.num_mels, config.sampling_rate, config.hop_size, config.win_size, config.fmin, config.fmax_for_loss)\n",
    "    optim_d.zero_grad() \n",
    "\n",
    "    y_df_hat_r, y_df_hat_g, _, _ = mpd(y, y_g_hat.detach()) \n",
    "    loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g) \n",
    "\n",
    "    y_ds_hat_r, y_ds_hat_g, _, _ = msd(y, y_g_hat.detach()) \n",
    "    loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g) \n",
    "\n",
    "    loss_disc_all = loss_disc_s + loss_disc_f \n",
    "\n",
    "    loss_disc_all.backward() \n",
    "    optim_d.step() \n",
    "    \n",
    "    optim_g.zero_grad() \n",
    "\n",
    "    loss_mel = F.l1_loss(y_mel, y_g_hat_mel.permute(2, 0, 1)) * 45 \n",
    "\n",
    "    y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mpd(y, y_g_hat) \n",
    "    y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = msd(y, y_g_hat) \n",
    "    loss_fm_f = feature_loss(fmap_f_r, fmap_f_g) \n",
    "    loss_fm_s = feature_loss(fmap_s_r, fmap_s_g) \n",
    "    loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g) \n",
    "    loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g) \n",
    "    loss_gen_all = loss_gen_s + loss_gen_f + loss_fm_s + loss_fm_f + loss_mel \n",
    "\n",
    "    loss_gen_all.backward() \n",
    "    optim_g.step() \n",
    "\n",
    "    writer.add_scalar('training/gen_loss_total', loss_gen_all.item(), global_step)\n",
    "    writer.add_scalar('training/mel_spec_error', loss_mel.item()/45, global_step) \n",
    "    writer.add_scalar('training/disc_loss_total', loss_disc_all.item(), global_step)\n",
    "\n",
    "    if global_step % 500 == 0: \n",
    "      test_loss = validation_loop(global_step) \n",
    "\n",
    "      if best_loss > test_loss: \n",
    "        best_loss = test_loss \n",
    "        torch.save(generator_model.state_dict(), 'models/gen_model')\n",
    "        torch.save(mpd.state_dict(), 'models/mpd')\n",
    "        torch.save(msd.state_dict(), 'models/msd')\n",
    "    \n",
    "    global_step += 1 \n",
    "\n",
    "    loop.set_description(f'Epoch {epoch+1}/{n_epochs}')\n",
    "    loop.set_postfix(loss_disc_all=loss_disc_all.item(), loss_gen_all=loss_gen_all.item(), best_loss=best_loss, current_test_loss=test_loss)\n",
    "\n",
    "  writer.add_scalar('learning/d_lr', scheduler_d.get_last_lr()[0], global_step)\n",
    "  writer.add_scalar('learning/g_lr', scheduler_g.get_last_lr()[0], global_step)\n",
    "  scheduler_d.step() \n",
    "  scheduler_g.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.258, current_test_loss=0.261, loss_disc_all=2.65, loss_gen_all=25.5]\n",
      "Epoch 102/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.258, current_test_loss=0.258, loss_disc_all=2.75, loss_gen_all=24.3]  \n",
      "Epoch 103/200: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.255, current_test_loss=0.255, loss_disc_all=2.88, loss_gen_all=24.9]\n",
      "Epoch 104/200: 100%|██████████| 736/736 [07:23<00:00,  1.66it/s, best_loss=0.255, current_test_loss=0.256, loss_disc_all=2.66, loss_gen_all=25]    \n",
      "Epoch 105/200: 100%|██████████| 736/736 [06:53<00:00,  1.78it/s, best_loss=0.255, current_test_loss=0.26, loss_disc_all=2.83, loss_gen_all=23.4] \n",
      "Epoch 106/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.255, current_test_loss=0.256, loss_disc_all=2.66, loss_gen_all=25]    \n",
      "Epoch 107/200: 100%|██████████| 736/736 [06:52<00:00,  1.78it/s, best_loss=0.253, current_test_loss=0.253, loss_disc_all=2.64, loss_gen_all=25.8]\n",
      "Epoch 108/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.253, current_test_loss=0.258, loss_disc_all=2.79, loss_gen_all=24.5]  \n",
      "Epoch 109/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.253, current_test_loss=0.258, loss_disc_all=2.9, loss_gen_all=24.1] \n",
      "Epoch 110/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.253, current_test_loss=0.256, loss_disc_all=2.78, loss_gen_all=25]    \n",
      "Epoch 111/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.253, current_test_loss=0.262, loss_disc_all=2.67, loss_gen_all=24.9] \n",
      "Epoch 112/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.253, current_test_loss=0.256, loss_disc_all=2.73, loss_gen_all=24.6]  \n",
      "Epoch 113/200: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.251, current_test_loss=0.251, loss_disc_all=2.64, loss_gen_all=24.3] \n",
      "Epoch 114/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.251, current_test_loss=0.252, loss_disc_all=2.67, loss_gen_all=25]    \n",
      "Epoch 115/200: 100%|██████████| 736/736 [07:26<00:00,  1.65it/s, best_loss=0.248, current_test_loss=0.25, loss_disc_all=2.75, loss_gen_all=24.3]  \n",
      "Epoch 116/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.248, current_test_loss=0.248, loss_disc_all=2.58, loss_gen_all=25.6]  \n",
      "Epoch 117/200: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.248, current_test_loss=0.248, loss_disc_all=2.76, loss_gen_all=25.9]  \n",
      "Epoch 118/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.248, current_test_loss=0.248, loss_disc_all=2.68, loss_gen_all=25.4]  \n",
      "Epoch 119/200: 100%|██████████| 736/736 [07:26<00:00,  1.65it/s, best_loss=0.247, current_test_loss=0.247, loss_disc_all=2.71, loss_gen_all=24.8]  \n",
      "Epoch 120/200: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.246, current_test_loss=0.246, loss_disc_all=2.63, loss_gen_all=25.3]  \n",
      "Epoch 121/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.246, current_test_loss=0.25, loss_disc_all=2.63, loss_gen_all=24.5]   \n",
      "Epoch 122/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.246, current_test_loss=0.249, loss_disc_all=2.86, loss_gen_all=25]  \n",
      "Epoch 123/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.246, current_test_loss=0.257, loss_disc_all=2.6, loss_gen_all=24.5]   \n",
      "Epoch 124/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.246, current_test_loss=0.256, loss_disc_all=2.82, loss_gen_all=23.7]\n",
      "Epoch 125/200: 100%|██████████| 736/736 [06:52<00:00,  1.78it/s, best_loss=0.245, current_test_loss=0.245, loss_disc_all=2.86, loss_gen_all=24]    \n",
      "Epoch 126/200: 100%|██████████| 736/736 [07:23<00:00,  1.66it/s, best_loss=0.245, current_test_loss=0.25, loss_disc_all=2.7, loss_gen_all=24.1]  \n",
      "Epoch 127/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.245, current_test_loss=0.251, loss_disc_all=2.65, loss_gen_all=24.6]  \n",
      "Epoch 128/200: 100%|██████████| 736/736 [07:23<00:00,  1.66it/s, best_loss=0.245, current_test_loss=0.247, loss_disc_all=2.58, loss_gen_all=26]   \n",
      "Epoch 129/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.245, current_test_loss=0.247, loss_disc_all=2.64, loss_gen_all=25.1]  \n",
      "Epoch 130/200: 100%|██████████| 736/736 [07:22<00:00,  1.66it/s, best_loss=0.245, current_test_loss=0.246, loss_disc_all=2.81, loss_gen_all=24.6] \n",
      "Epoch 131/200: 100%|██████████| 736/736 [06:52<00:00,  1.78it/s, best_loss=0.24, current_test_loss=0.24, loss_disc_all=2.73, loss_gen_all=25.9]  \n",
      "Epoch 132/200: 100%|██████████| 736/736 [07:22<00:00,  1.66it/s, best_loss=0.24, current_test_loss=0.249, loss_disc_all=2.77, loss_gen_all=23.6] \n",
      "Epoch 133/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.24, current_test_loss=0.252, loss_disc_all=2.75, loss_gen_all=24.4]  \n",
      "Epoch 134/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.24, current_test_loss=0.241, loss_disc_all=2.77, loss_gen_all=25.6]  \n",
      "Epoch 135/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.24, current_test_loss=0.247, loss_disc_all=2.73, loss_gen_all=25.2]  \n",
      "Epoch 136/200: 100%|██████████| 736/736 [07:23<00:00,  1.66it/s, best_loss=0.24, current_test_loss=0.247, loss_disc_all=2.67, loss_gen_all=24.8]  \n",
      "Epoch 137/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.24, current_test_loss=0.253, loss_disc_all=2.7, loss_gen_all=25]   \n",
      "Epoch 138/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.24, current_test_loss=0.253, loss_disc_all=2.61, loss_gen_all=25.8]  \n",
      "Epoch 139/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.24, current_test_loss=0.25, loss_disc_all=2.56, loss_gen_all=24.8] \n",
      "Epoch 140/200: 100%|██████████| 736/736 [07:23<00:00,  1.66it/s, best_loss=0.24, current_test_loss=0.252, loss_disc_all=2.55, loss_gen_all=25.4]  \n",
      "Epoch 141/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.24, current_test_loss=0.265, loss_disc_all=2.64, loss_gen_all=24.8]\n",
      "Epoch 142/200: 100%|██████████| 736/736 [07:24<00:00,  1.65it/s, best_loss=0.24, current_test_loss=0.249, loss_disc_all=2.83, loss_gen_all=24.1]  \n",
      "Epoch 143/200: 100%|██████████| 736/736 [06:49<00:00,  1.80it/s, best_loss=0.24, current_test_loss=0.241, loss_disc_all=2.77, loss_gen_all=24.9]\n",
      "Epoch 144/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.24, current_test_loss=0.25, loss_disc_all=2.63, loss_gen_all=24.4]  \n",
      "Epoch 145/200: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.24, current_test_loss=0.25, loss_disc_all=2.69, loss_gen_all=24.9]  \n",
      "Epoch 146/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.24, current_test_loss=0.244, loss_disc_all=2.71, loss_gen_all=25.6]  \n",
      "Epoch 147/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.24, current_test_loss=0.25, loss_disc_all=2.59, loss_gen_all=25]    \n",
      "Epoch 148/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.24, current_test_loss=0.245, loss_disc_all=2.66, loss_gen_all=24.2]  \n",
      "Epoch 149/200: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.24, current_test_loss=0.248, loss_disc_all=2.8, loss_gen_all=24.9]  \n",
      "Epoch 150/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.24, current_test_loss=0.246, loss_disc_all=2.72, loss_gen_all=24.9]  \n",
      "Epoch 151/200: 100%|██████████| 736/736 [07:26<00:00,  1.65it/s, best_loss=0.239, current_test_loss=0.239, loss_disc_all=2.68, loss_gen_all=24.1] \n",
      "Epoch 152/200: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.239, current_test_loss=0.247, loss_disc_all=2.76, loss_gen_all=24.7]  \n",
      "Epoch 153/200: 100%|██████████| 736/736 [07:24<00:00,  1.65it/s, best_loss=0.239, current_test_loss=0.241, loss_disc_all=2.68, loss_gen_all=24.7] \n",
      "Epoch 154/200: 100%|██████████| 736/736 [06:54<00:00,  1.78it/s, best_loss=0.238, current_test_loss=0.238, loss_disc_all=2.81, loss_gen_all=24.5]  \n",
      "Epoch 155/200: 100%|██████████| 736/736 [07:24<00:00,  1.65it/s, best_loss=0.238, current_test_loss=0.247, loss_disc_all=2.72, loss_gen_all=24.2]  \n",
      "Epoch 156/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.238, current_test_loss=0.254, loss_disc_all=2.7, loss_gen_all=25.3] \n",
      "Epoch 157/200: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.238, current_test_loss=0.243, loss_disc_all=2.78, loss_gen_all=24.7]  \n",
      "Epoch 158/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.238, current_test_loss=0.241, loss_disc_all=2.85, loss_gen_all=25.5]\n",
      "Epoch 159/200: 100%|██████████| 736/736 [07:25<00:00,  1.65it/s, best_loss=0.238, current_test_loss=0.248, loss_disc_all=2.72, loss_gen_all=24.2]  \n",
      "Epoch 160/200: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.238, current_test_loss=0.25, loss_disc_all=2.7, loss_gen_all=25.6]  \n",
      "Epoch 161/200: 100%|██████████| 736/736 [06:50<00:00,  1.79it/s, best_loss=0.238, current_test_loss=0.244, loss_disc_all=2.68, loss_gen_all=24.6]  \n",
      "Epoch 162/200: 100%|██████████| 736/736 [07:24<00:00,  1.66it/s, best_loss=0.238, current_test_loss=0.247, loss_disc_all=2.76, loss_gen_all=24.5]\n",
      "Epoch 163/200: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.238, current_test_loss=0.24, loss_disc_all=2.66, loss_gen_all=25.5]  \n",
      "Epoch 164/200: 100%|██████████| 736/736 [07:26<00:00,  1.65it/s, best_loss=0.238, current_test_loss=0.238, loss_disc_all=2.78, loss_gen_all=25]   \n",
      "Epoch 165/200: 100%|██████████| 736/736 [06:51<00:00,  1.79it/s, best_loss=0.238, current_test_loss=0.247, loss_disc_all=2.71, loss_gen_all=23.5]  \n",
      "Epoch 166/200: 100%|██████████| 736/736 [07:26<00:00,  1.65it/s, best_loss=0.238, current_test_loss=0.239, loss_disc_all=2.78, loss_gen_all=24.4] \n",
      "Epoch 167/200: 100%|██████████| 736/736 [07:05<00:00,  1.73it/s, best_loss=0.238, current_test_loss=0.239, loss_disc_all=2.8, loss_gen_all=24.3]   \n",
      "Epoch 168/200: 100%|██████████| 736/736 [07:56<00:00,  1.55it/s, best_loss=0.238, current_test_loss=0.242, loss_disc_all=2.72, loss_gen_all=24.3]\n",
      "Epoch 169/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.238, current_test_loss=0.244, loss_disc_all=2.79, loss_gen_all=24.9]\n",
      "Epoch 170/200: 100%|██████████| 736/736 [07:56<00:00,  1.54it/s, best_loss=0.238, current_test_loss=0.244, loss_disc_all=2.81, loss_gen_all=24.6]  \n",
      "Epoch 171/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.238, current_test_loss=0.247, loss_disc_all=2.75, loss_gen_all=24.3]  \n",
      "Epoch 172/200: 100%|██████████| 736/736 [07:57<00:00,  1.54it/s, best_loss=0.234, current_test_loss=0.249, loss_disc_all=2.8, loss_gen_all=24.4]   \n",
      "Epoch 173/200: 100%|██████████| 736/736 [07:19<00:00,  1.68it/s, best_loss=0.234, current_test_loss=0.241, loss_disc_all=2.64, loss_gen_all=24.4] \n",
      "Epoch 174/200: 100%|██████████| 736/736 [07:56<00:00,  1.55it/s, best_loss=0.234, current_test_loss=0.238, loss_disc_all=2.64, loss_gen_all=25.5] \n",
      "Epoch 175/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.234, current_test_loss=0.239, loss_disc_all=2.69, loss_gen_all=24.8]\n",
      "Epoch 176/200: 100%|██████████| 736/736 [07:56<00:00,  1.54it/s, best_loss=0.234, current_test_loss=0.242, loss_disc_all=2.67, loss_gen_all=24.2]  \n",
      "Epoch 177/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.234, current_test_loss=0.25, loss_disc_all=2.81, loss_gen_all=24.7] \n",
      "Epoch 178/200: 100%|██████████| 736/736 [07:56<00:00,  1.54it/s, best_loss=0.234, current_test_loss=0.241, loss_disc_all=2.7, loss_gen_all=23.9]   \n",
      "Epoch 179/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.234, current_test_loss=0.238, loss_disc_all=2.82, loss_gen_all=23.3]\n",
      "Epoch 180/200: 100%|██████████| 736/736 [07:19<00:00,  1.68it/s, best_loss=0.234, current_test_loss=0.24, loss_disc_all=2.72, loss_gen_all=24.5]  \n",
      "Epoch 181/200: 100%|██████████| 736/736 [07:56<00:00,  1.55it/s, best_loss=0.234, current_test_loss=0.241, loss_disc_all=2.89, loss_gen_all=23.9] \n",
      "Epoch 182/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.234, current_test_loss=0.235, loss_disc_all=2.9, loss_gen_all=25.2]   \n",
      "Epoch 183/200: 100%|██████████| 736/736 [07:57<00:00,  1.54it/s, best_loss=0.234, current_test_loss=0.244, loss_disc_all=2.62, loss_gen_all=25]   \n",
      "Epoch 184/200: 100%|██████████| 736/736 [07:19<00:00,  1.67it/s, best_loss=0.234, current_test_loss=0.24, loss_disc_all=2.77, loss_gen_all=24.7]  \n",
      "Epoch 185/200: 100%|██████████| 736/736 [07:55<00:00,  1.55it/s, best_loss=0.234, current_test_loss=0.24, loss_disc_all=2.84, loss_gen_all=23.8]  \n",
      "Epoch 186/200: 100%|██████████| 736/736 [07:21<00:00,  1.67it/s, best_loss=0.233, current_test_loss=0.233, loss_disc_all=2.64, loss_gen_all=24.9]  \n",
      "Epoch 187/200: 100%|██████████| 736/736 [07:55<00:00,  1.55it/s, best_loss=0.233, current_test_loss=0.235, loss_disc_all=2.84, loss_gen_all=24.1] \n",
      "Epoch 188/200: 100%|██████████| 736/736 [07:20<00:00,  1.67it/s, best_loss=0.233, current_test_loss=0.242, loss_disc_all=2.61, loss_gen_all=25.4]  \n",
      "Epoch 189/200: 100%|██████████| 736/736 [07:56<00:00,  1.55it/s, best_loss=0.233, current_test_loss=0.239, loss_disc_all=2.78, loss_gen_all=24.1] \n",
      "Epoch 190/200: 100%|██████████| 736/736 [07:21<00:00,  1.67it/s, best_loss=0.233, current_test_loss=0.236, loss_disc_all=2.67, loss_gen_all=24.4]  \n",
      "Epoch 191/200: 100%|██████████| 736/736 [07:55<00:00,  1.55it/s, best_loss=0.233, current_test_loss=0.239, loss_disc_all=2.7, loss_gen_all=24.4]   \n",
      "Epoch 192/200: 100%|██████████| 736/736 [07:23<00:00,  1.66it/s, best_loss=0.233, current_test_loss=0.233, loss_disc_all=2.65, loss_gen_all=24.5]  \n",
      "Epoch 193/200: 100%|██████████| 736/736 [07:57<00:00,  1.54it/s, best_loss=0.233, current_test_loss=0.241, loss_disc_all=2.68, loss_gen_all=25.4]  \n",
      "Epoch 194/200: 100%|██████████| 736/736 [07:05<00:00,  1.73it/s, best_loss=0.233, current_test_loss=0.239, loss_disc_all=2.65, loss_gen_all=25]  \n",
      "Epoch 195/200: 100%|██████████| 736/736 [07:45<00:00,  1.58it/s, best_loss=0.233, current_test_loss=0.236, loss_disc_all=2.69, loss_gen_all=25]   \n",
      "Epoch 196/200: 100%|██████████| 736/736 [07:07<00:00,  1.72it/s, best_loss=0.233, current_test_loss=0.244, loss_disc_all=2.65, loss_gen_all=25.4]\n",
      "Epoch 197/200: 100%|██████████| 736/736 [07:04<00:00,  1.73it/s, best_loss=0.233, current_test_loss=0.239, loss_disc_all=2.65, loss_gen_all=24.6]  \n",
      "Epoch 198/200: 100%|██████████| 736/736 [07:41<00:00,  1.60it/s, best_loss=0.233, current_test_loss=0.233, loss_disc_all=2.65, loss_gen_all=24.5] \n",
      "Epoch 199/200: 100%|██████████| 736/736 [07:04<00:00,  1.73it/s, best_loss=0.233, current_test_loss=0.234, loss_disc_all=2.75, loss_gen_all=24.4]  \n",
      "Epoch 200/200: 100%|██████████| 736/736 [07:38<00:00,  1.61it/s, best_loss=0.233, current_test_loss=0.247, loss_disc_all=2.7, loss_gen_all=24.8]  \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(100, n_epochs): \n",
    "  for i, batch in (loop := tqdm(enumerate(train_dl), total=len(train_dl))): \n",
    "    x, y, _, y_mel = batch \n",
    "    x, y, y_mel = x.permute(0, 2, 1).to(device), y.unsqueeze(1).to(device), y_mel.to(device)\n",
    "\n",
    "    y_g_hat = generator_model(x) \n",
    "    y_g_hat_mel = dataset.mel_spectrogram(y_g_hat.squeeze(1), config.n_fft, config.num_mels, config.sampling_rate, config.hop_size, config.win_size, config.fmin, config.fmax_for_loss)\n",
    "    optim_d.zero_grad() \n",
    "\n",
    "    y_df_hat_r, y_df_hat_g, _, _ = mpd(y, y_g_hat.detach()) \n",
    "    loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g) \n",
    "\n",
    "    y_ds_hat_r, y_ds_hat_g, _, _ = msd(y, y_g_hat.detach()) \n",
    "    loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g) \n",
    "\n",
    "    loss_disc_all = loss_disc_s + loss_disc_f \n",
    "\n",
    "    loss_disc_all.backward() \n",
    "    optim_d.step() \n",
    "    \n",
    "    optim_g.zero_grad() \n",
    "\n",
    "    loss_mel = F.l1_loss(y_mel, y_g_hat_mel.permute(2, 0, 1)) * 45 \n",
    "\n",
    "    y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mpd(y, y_g_hat) \n",
    "    y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = msd(y, y_g_hat) \n",
    "    loss_fm_f = feature_loss(fmap_f_r, fmap_f_g) \n",
    "    loss_fm_s = feature_loss(fmap_s_r, fmap_s_g) \n",
    "    loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g) \n",
    "    loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g) \n",
    "    loss_gen_all = loss_gen_s + loss_gen_f + loss_fm_s + loss_fm_f + loss_mel \n",
    "\n",
    "    loss_gen_all.backward() \n",
    "    optim_g.step() \n",
    "\n",
    "    writer.add_scalar('training/gen_loss_total', loss_gen_all.item(), global_step)\n",
    "    writer.add_scalar('training/mel_spec_error', loss_mel.item()/45, global_step) \n",
    "    writer.add_scalar('training/disc_loss_total', loss_disc_all.item(), global_step)\n",
    "\n",
    "    if global_step % 500 == 0: \n",
    "      test_loss = validation_loop(global_step) \n",
    "\n",
    "      if best_loss > test_loss: \n",
    "        best_loss = test_loss \n",
    "        torch.save(generator_model.state_dict(), 'models/gen_model')\n",
    "        torch.save(mpd.state_dict(), 'models/mpd')\n",
    "        torch.save(msd.state_dict(), 'models/msd')\n",
    "    \n",
    "    global_step += 1 \n",
    "\n",
    "    loop.set_description(f'Epoch {epoch+1}/{n_epochs}')\n",
    "    loop.set_postfix(loss_disc_all=loss_disc_all.item(), loss_gen_all=loss_gen_all.item(), best_loss=best_loss, current_test_loss=test_loss)\n",
    "\n",
    "  writer.add_scalar('learning/d_lr', scheduler_d.get_last_lr()[0], global_step)\n",
    "  writer.add_scalar('learning/g_lr', scheduler_g.get_last_lr()[0], global_step)\n",
    "  scheduler_d.step() \n",
    "  scheduler_g.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
